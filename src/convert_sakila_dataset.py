#!/usr/bin/env python3
"""
Sakilaæ•°æ®é›†è½¬æ¢è„šæœ¬
å°†åŸå§‹çš„q2sql_pairs.jsonè½¬æ¢ä¸ºè¯„ä¼°ç³»ç»Ÿéœ€è¦çš„æ ¼å¼ï¼Œå¹¶è‡ªåŠ¨æ¨æ–­è¡¨å’Œåˆ—ä¿¡æ¯
"""

import json
import re
import yaml
from typing import Dict, List, Set, Any
from pathlib import Path

class SakilaDatasetConverter:
    def __init__(self, ddl_file: str, db_description_file: str):
        """åˆå§‹åŒ–è½¬æ¢å™¨"""
        self.ddl_file = ddl_file
        self.db_description_file = db_description_file
        self.table_info = {}
        self.column_info = {}
        self.load_schema_info()
    
    def load_schema_info(self):
        """åŠ è½½æ•°æ®åº“æ¨¡å¼ä¿¡æ¯"""
        # åŠ è½½DDLä¿¡æ¯
        with open(self.ddl_file, 'r', encoding='utf-8') as f:
            ddl_data = yaml.safe_load(f)
        
        # åŠ è½½æè¿°ä¿¡æ¯
        with open(self.db_description_file, 'r', encoding='utf-8') as f:
            desc_data = yaml.safe_load(f)
        
        # è§£æè¡¨å’Œåˆ—ä¿¡æ¯
        for table_name, ddl in ddl_data.items():
            if 'CREATE TABLE' in ddl:  # åªå¤„ç†è¡¨ï¼Œä¸å¤„ç†è§†å›¾
                self.table_info[table_name] = {
                    'columns': self.extract_columns_from_ddl(ddl),
                    'primary_keys': self.extract_primary_keys_from_ddl(ddl),
                    'description': desc_data.get(table_name, {})
                }
    
    def extract_columns_from_ddl(self, ddl: str) -> List[str]:
        """ä»DDLä¸­æå–åˆ—å"""
        columns = []
        lines = ddl.split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith('`') and not line.startswith('PRIMARY KEY') and not line.startswith('KEY') and not line.startswith('CONSTRAINT'):
                # æå–åˆ—å
                match = re.match(r'`([^`]+)`', line)
                if match:
                    columns.append(match.group(1))
        return columns
    
    def extract_primary_keys_from_ddl(self, ddl: str) -> List[str]:
        """ä»DDLä¸­æå–ä¸»é”®"""
        primary_keys = []
        # æŸ¥æ‰¾PRIMARY KEYå®šä¹‰
        pk_match = re.search(r'PRIMARY KEY \(`([^`]+)`\)', ddl)
        if pk_match:
            primary_keys.append(pk_match.group(1))
        return primary_keys
    
    def extract_tables_from_sql(self, sql: str) -> Set[str]:
        """ä»SQLè¯­å¥ä¸­æå–è¡¨å"""
        tables = set()
        sql_upper = sql.upper()
        
        # å¸¸è§çš„è¡¨åæå–æ¨¡å¼
        patterns = [
            r'\bFROM\s+`?(\w+)`?',
            r'\bJOIN\s+`?(\w+)`?',
            r'\bINTO\s+`?(\w+)`?',
            r'\bUPDATE\s+`?(\w+)`?',
            r'\bDELETE\s+FROM\s+`?(\w+)`?'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, sql_upper)
            for match in matches:
                table_name = match.lower()
                if table_name in self.table_info:
                    tables.add(table_name)
        
        return tables
    
    def extract_columns_from_sql(self, sql: str, tables: Set[str]) -> Set[str]:
        """ä»SQLè¯­å¥ä¸­æå–åˆ—å"""
        columns = set()
        sql_upper = sql.upper()
        
        # æå–SELECTå­å¥ä¸­çš„åˆ—
        select_match = re.search(r'SELECT\s+(.*?)\s+FROM', sql_upper)
        if select_match:
            select_clause = select_match.group(1)
            if select_clause.strip() != '*':
                # åˆ†å‰²åˆ—å
                col_parts = select_clause.split(',')
                for part in col_parts:
                    part = part.strip()
                    # ç§»é™¤åˆ«å
                    if ' AS ' in part:
                        part = part.split(' AS ')[0].strip()
                    # ç§»é™¤è¡¨å‰ç¼€
                    if '.' in part:
                        part = part.split('.')[-1]
                    # ç§»é™¤åå¼•å·
                    part = part.replace('`', '').lower()
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆåˆ—å
                    for table in tables:
                        if part in self.table_info[table]['columns']:
                            columns.add(part)
        
        # æå–WHEREå­å¥ä¸­çš„åˆ—
        where_patterns = [
            r'WHERE\s+`?(\w+)`?\s*[=<>!]',
            r'AND\s+`?(\w+)`?\s*[=<>!]',
            r'OR\s+`?(\w+)`?\s*[=<>!]'
        ]
        
        for pattern in where_patterns:
            matches = re.findall(pattern, sql_upper)
            for match in matches:
                col_name = match.lower()
                for table in tables:
                    if col_name in self.table_info[table]['columns']:
                        columns.add(col_name)
        
        return columns
    
    def identify_key_columns(self, tables: Set[str], columns: Set[str]) -> Set[str]:
        """è¯†åˆ«å…³é”®åˆ—ï¼ˆä¸»é”®ã€å¤–é”®ç­‰ï¼‰"""
        key_columns = set()
        
        for table in tables:
            table_info = self.table_info[table]
            # æ·»åŠ ä¸»é”®
            for pk in table_info['primary_keys']:
                if pk in columns:
                    key_columns.add(pk)
            
            # æ·»åŠ IDåˆ—ï¼ˆé€šå¸¸æ˜¯å…³é”®åˆ—ï¼‰
            for col in columns:
                if col.endswith('_id') or col == 'id':
                    key_columns.add(col)
        
        return key_columns
    
    def convert_dataset(self, input_file: str, output_file: str):
        """è½¬æ¢æ•°æ®é›†"""
        # è¯»å–åŸå§‹æ•°æ®
        with open(input_file, 'r', encoding='utf-8') as f:
            original_data = json.load(f)
        
        converted_data = []
        
        for i, item in enumerate(original_data, 1):
            question = item['question']
            sql = item['sql']
            
            # æå–è¡¨å’Œåˆ—ä¿¡æ¯
            tables = self.extract_tables_from_sql(sql)
            columns = self.extract_columns_from_sql(sql, tables)
            key_columns = self.identify_key_columns(tables, columns)
            
            # æ„å»ºè½¬æ¢åçš„æ•°æ®é¡¹
            converted_item = {
                "id": i,
                "question": question,
                "expected_sql": sql,
                "expected_tables": list(tables),
                "expected_columns": list(columns),
                "key_columns": list(key_columns),
                "difficulty": self.assess_difficulty(sql),
                "sql_type": self.identify_sql_type(sql),
                "metadata": {
                    "source": "sakila",
                    "domain": "video_rental",
                    "complexity": len(tables) + len(columns)
                }
            }
            
            converted_data.append(converted_item)
        
        # ä¿å­˜è½¬æ¢åçš„æ•°æ®
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(converted_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… æ•°æ®é›†è½¬æ¢å®Œæˆï¼")
        print(f"   åŸå§‹æ•°æ®: {len(original_data)} æ¡")
        print(f"   è½¬æ¢åæ•°æ®: {len(converted_data)} æ¡")
        print(f"   è¾“å‡ºæ–‡ä»¶: {output_file}")
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self.print_statistics(converted_data)
    
    def assess_difficulty(self, sql: str) -> str:
        """è¯„ä¼°SQLéš¾åº¦"""
        sql_upper = sql.upper()
        
        # ç®€å•ï¼šå•è¡¨æŸ¥è¯¢
        if 'JOIN' not in sql_upper and 'SUBQUERY' not in sql_upper:
            return "easy"
        
        # å›°éš¾ï¼šåŒ…å«JOINæˆ–å­æŸ¥è¯¢
        if 'JOIN' in sql_upper or 'SELECT' in sql_upper.count('SELECT') > 1:
            return "hard"
        
        # ä¸­ç­‰ï¼šå…¶ä»–æƒ…å†µ
        return "medium"
    
    def identify_sql_type(self, sql: str) -> str:
        """è¯†åˆ«SQLç±»å‹"""
        sql_upper = sql.upper().strip()
        
        if sql_upper.startswith('SELECT'):
            return "SELECT"
        elif sql_upper.startswith('INSERT'):
            return "INSERT"
        elif sql_upper.startswith('UPDATE'):
            return "UPDATE"
        elif sql_upper.startswith('DELETE'):
            return "DELETE"
        else:
            return "OTHER"
    
    def print_statistics(self, data: List[Dict[str, Any]]):
        """æ‰“å°æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        print("\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯:")
        
        # SQLç±»å‹ç»Ÿè®¡
        sql_types = {}
        difficulties = {}
        
        for item in data:
            sql_type = item['sql_type']
            difficulty = item['difficulty']
            
            sql_types[sql_type] = sql_types.get(sql_type, 0) + 1
            difficulties[difficulty] = difficulties.get(difficulty, 0) + 1
        
        print(f"   SQLç±»å‹åˆ†å¸ƒ: {sql_types}")
        print(f"   éš¾åº¦åˆ†å¸ƒ: {difficulties}")
        
        # è¡¨ä½¿ç”¨ç»Ÿè®¡
        table_usage = {}
        for item in data:
            for table in item['expected_tables']:
                table_usage[table] = table_usage.get(table, 0) + 1
        
        print(f"   æœ€å¸¸ç”¨çš„è¡¨: {sorted(table_usage.items(), key=lambda x: x[1], reverse=True)[:5]}")

def main():
    """ä¸»å‡½æ•°"""
    # æ–‡ä»¶è·¯å¾„
    base_dir = Path(__file__).parent.parent.parent  # å›åˆ°é¡¹ç›®æ ¹ç›®å½•
    ddl_file = base_dir / "Data" / "sakila" / "ddl_statements.yaml"
    db_desc_file = base_dir / "Data" / "sakila" / "db_description.yaml"
    input_file = base_dir / "Data" / "sakila" / "q2sql_pairs.json"
    output_file = base_dir / "Evaluation" / "datasets" / "sakila_real.json"
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_file.parent.mkdir(exist_ok=True)
    
    # åˆ›å»ºè½¬æ¢å™¨å¹¶æ‰§è¡Œè½¬æ¢
    converter = SakilaDatasetConverter(str(ddl_file), str(db_desc_file))
    converter.convert_dataset(str(input_file), str(output_file))

if __name__ == "__main__":
    main() 